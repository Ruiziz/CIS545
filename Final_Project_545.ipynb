{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "name": "Final Project 545",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dkq9oiyfPQEr",
        "nbgrader": {
          "grade": false,
          "grade_id": "title",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "# CIS 545 Final Project: Amazon Review Analysis and Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC4GF9zcUkOs",
        "colab_type": "text"
      },
      "source": [
        "In hw4, we have finished some data processing jobs (like PCA or TF-IDF scores) and also tried several models, such like Random Forest, t-SNE and k-means to predict the rating of the reviews. But all the results are not so satisfying. Therefore, in this project, we will continually did. some work to improve the accuracy of our model. To achieve this goal, this project would be divided into five main parts below (including the preparing section): \n",
        "\n",
        "0.   Preparing section: download the raw data and inherit somr required data from previous homework.\n",
        "\n",
        "1.   Hyperparameter Tunning for Random Forest: try to improve the modeling for choosing different value of parameters.\n",
        "\n",
        "2.   Try several frequently used model and see if the accuracy was improved.\n",
        "3.   Neural Network: in this part, we tried two ways: one simple NN and the other is LSTMs.\n",
        "\n",
        "\n",
        "4.   how date of the reviews will influence the prediction accuracy of models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nB-65mi_PQEu",
        "colab": {}
      },
      "source": [
        "# install stuff\n",
        "%%capture\n",
        "!pip install -U gensim\n",
        "!pip install urllib2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4LDqswUUjIA",
        "colab_type": "text"
      },
      "source": [
        "Make sure the following line prints the up-to-date version of `gensim`, which at time of releasing this homework was version 3.8.1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XCZ2LpmRcSIX",
        "outputId": "1f146b29-70cb-4f74-83f2-c5061408f374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "# check gensim version\n",
        "import gensim\n",
        "gensim.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.8.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ngg1pmoZPQE4",
        "colab": {}
      },
      "source": [
        "# import stuff\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel, KeyedVectors\n",
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "from gensim.models.nmf import Nmf\n",
        "\n",
        "import sklearn.model_selection as ms\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from datetime import *\n",
        "from operator import itemgetter\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79R84xo4FESL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!wget https://cis.upenn.edu/~cis545/data/reviews.dict\n",
        "!wget https://cis.upenn.edu/~cis545/data/train_reviews.mm\n",
        "!wget https://cis.upenn.edu/~cis545/data/train_times.npy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vBLPPKcePQE9",
        "colab": {}
      },
      "source": [
        "# get the raw dataset\n",
        "reviews_dict = corpora.Dictionary.load(\"reviews.dict\")\n",
        "reviews_bow = corpora.MmCorpus('train_reviews.mm')\n",
        "reviews_times  = np.load('train_times.npy')\n",
        "reviews_times.shape = (len(reviews_bow),1)\n",
        "y = np.vstack((np.repeat(1, 4000), np.repeat(2, 4000), np.repeat(3, 4000), np.repeat(4, 4000), np.repeat(5, 4000))) # 4000 * 5 ndarray\n",
        "y = np.repeat(y, 5) # (4000 * 5 * 5) * 1 (1D-ndarray)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1KvapatWrcY",
        "colab_type": "text"
      },
      "source": [
        "###Step 0: inherit the required data from hw4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIPfJkDoXArc",
        "colab_type": "text"
      },
      "source": [
        "### step 0.0: load the required function from hw4\n",
        "\n",
        "##### step: 0.0.1: functions for processing the text contents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-s5GeN5SKq2o",
        "nbgrader": {
          "grade": false,
          "grade_id": "answer-0-2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {}
      },
      "source": [
        "# functions about reviews\n",
        "\n",
        "def lookup_docs(corpus, indices):\n",
        "  idx = 0\n",
        "  new_corpus = []\n",
        "  for doc in corpus:\n",
        "      if (idx in indices):\n",
        "        new_corpus.append(doc)\n",
        "      idx += 1\n",
        "  return new_corpus\n",
        "\n",
        "\n",
        "def translate_review(review, reviews_dict):\n",
        "    Line = \"\"\n",
        "    for (idx, times) in review:\n",
        "      for i in range(int(times)):\n",
        "        Line += reviews_dict.get(idx, \"\") + \" \"\n",
        "    return Line\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97KRQ8nkgyty",
        "colab_type": "text"
      },
      "source": [
        "#### step: 0.0.2: functions for processing date datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2AvvxOpjPQG9",
        "nbgrader": {
          "grade": false,
          "grade_id": "answer-0-4-1",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "# functions about reviews' time\n",
        "\n",
        "def convert_times(reviews_times):\n",
        "  # return reviews_times\n",
        "  date_series = pd.Series([])\n",
        "  for time in reviews_times:\n",
        "    date = datetime.fromtimestamp(time)\n",
        "    date_series = date_series.append(pd.Series(date), ignore_index = True)\n",
        "  return date_series\n",
        "\n",
        "\n",
        "def days_before(time_item, offset):\n",
        "  from datetime import timedelta\n",
        "  time_sub = timedelta(days = offset)\n",
        "  before_date = time_item - time_sub\n",
        "  return before_date\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6BnapzzhMQC",
        "colab_type": "text"
      },
      "source": [
        "#### step: 0.0.3: functions for converting and representing data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dhlmCiiTzgph",
        "nbgrader": {
          "grade": false,
          "grade_id": "answer-1-2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {}
      },
      "source": [
        "# function to transfer raw input data to tfidf score\n",
        "def make_tfidf(reviews_bow):\n",
        "  \n",
        "    # initialize and fit a TF-IDF model to reviews_bow.\n",
        "    model = gensim.models.tfidfmodel.TfidfModel(corpus=reviews_bow, id2word=None, dictionary=None,                             \n",
        "                                                      normalize=True, smartirs=None, pivot=None, slope=0.25)\n",
        "    # apply TF-IDF model to reviews_bow to transform it\n",
        "    output = model[reviews_bow] #横轴是词出现的频率，纵轴是单词个数，一直在压缩横轴\n",
        "    return output\n",
        "\n",
        "# function for densifing\n",
        "def densify(sparse, columns):\n",
        "\n",
        "    array = np.zeros((len(sparse), columns))\n",
        "    row = 0\n",
        "    for line in sparse:\n",
        "      for(idx, times) in line:\n",
        "        array[row][idx] = times\n",
        "      row += 1\n",
        "    return array\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPwEvJAht4tO",
        "colab_type": "text"
      },
      "source": [
        "### step 0.1: convert the data to expected format\n",
        "\n",
        "#### step 0.1.1: convert the raw data to tf-idf scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vFNBR3JTZDYs",
        "nbgrader": {
          "grade": true,
          "grade_id": "visible-test-1-2",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {}
      },
      "source": [
        "reviews_tfidf = make_tfidf(reviews_bow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu6sSJLzGsVH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "#### step 0.1.2: reduce the dimensions of data to 40-dimensions \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI23DVA-qVxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cutoff = 40\n",
        "np.random.seed(1911)\n",
        "model = LsiModel(reviews_tfidf, id2word = reviews_dict, num_topics = cutoff)\n",
        "V = model[reviews_tfidf]\n",
        "X = densify(V, cutoff)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETk4AHFMgRCF",
        "colab_type": "text"
      },
      "source": [
        "#### step 0.1.3: change the data format to a human readable form and combine the reviews and corresponding rates into a same dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgkYmQTXa280",
        "colab_type": "code",
        "outputId": "bfa814e6-622f-49a2-fef8-76e4c1646c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "docs = lookup_docs(reviews_bow, range(len(reviews_bow)))\n",
        "readable_docs = []\n",
        "for doc in docs:\n",
        "  readable_doc = translate_review(doc, reviews_dict)\n",
        "  readable_docs.append(readable_doc)\n",
        "\n",
        "d_2w =  {\"Rate\": y, \"Text\" : readable_docs}\n",
        "df_2w = pd.DataFrame(d_2w)\n",
        "display(df_2w)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rate</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>abl aggress also apart apart aussi away ball b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>bark bark bark clean day dish dish dog dump fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>better disappoint money think wast absorb abso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>just will full next one price add anyth bottl ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>left now either go goe light light nowher plu ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>5</td>\n",
              "      <td>come tri will replac worth time work batteri m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>5</td>\n",
              "      <td>great use water water water problem straight e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>5</td>\n",
              "      <td>think one now go get around thing even small y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>5</td>\n",
              "      <td>abl littl work still well product realli remai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>5</td>\n",
              "      <td>better sinc use problem get top need help back...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Rate                                               Text\n",
              "0         1  abl aggress also apart apart aussi away ball b...\n",
              "1         1  bark bark bark clean day dish dish dog dump fi...\n",
              "2         1  better disappoint money think wast absorb abso...\n",
              "3         1  just will full next one price add anyth bottl ...\n",
              "4         1  left now either go goe light light nowher plu ...\n",
              "...     ...                                                ...\n",
              "99995     5  come tri will replac worth time work batteri m...\n",
              "99996     5  great use water water water problem straight e...\n",
              "99997     5  think one now go get around thing even small y...\n",
              "99998     5  abl littl work still well product realli remai...\n",
              "99999     5  better sinc use problem get top need help back...\n",
              "\n",
              "[100000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Rn49PaMlY-M",
        "nbgrader": {
          "grade": false,
          "grade_id": "spec-3",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "## Step 1: improve the performance of existed Ramdom Forest Model\n",
        "In hw4, we have got the review TF-IDF scores and the initialized a random forest with 70 estimators and 2-40 as range of cutoff values.  \n",
        "\n",
        "1.   We have tried the number between 2-40 as the cutoff value. In this part, we will try different cutoff values (larger than 40) and see if it will improve the accuracy efficiently. \n",
        "\n",
        "2.   In hw4, we have tried the number of estimators as 70, we will continually tried some other numbers below and see if these two parameters will cause large improvements of our model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qAbvT5iqrCI8",
        "nbgrader": {
          "grade": false,
          "grade_id": "spec-3-1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Step 1.1: inherit the required functions from hw4 and do a little edition.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "acWFX8EsrCdM",
        "nbgrader": {
          "grade": false,
          "grade_id": "answer-3-1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "colab": {}
      },
      "source": [
        "# function for random forest to get accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "def evaluate_model(X, review_times, y, estimators_num):\n",
        "    X = np.hstack((X, review_times))\n",
        "    X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state = 1911)\n",
        "    rfor = RandomForestClassifier(n_estimators=estimators_num, random_state=1911)\n",
        " \n",
        "    rfor_model = rfor.fit(X_train, y_train)\n",
        "    y_pred = rfor_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_pred, y_test)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "# function to choose cutoff value\n",
        "def evaluate_cutoffs(X_orig, X_dict, X_times, y, cutoffs, estimators_num):\n",
        "    results = []\n",
        "    for cutoff in cutoffs:\n",
        "        np.random.seed(1911)\n",
        "        # train an IsiModel\n",
        "        model = LsiModel(X_orig, id2word=X_dict, num_topics=cutoff)\n",
        "        # compute the V matrix\n",
        "        V_S = model[X_orig]\n",
        "        # call densify\n",
        "        densify_X = densify(V_S, cutoff)\n",
        "        # compute the accuracy and add it to the list\n",
        "        accuracy = evaluate_model(densify_X, X_times, y, estimators_num)\n",
        "        results.append(accuracy)\n",
        "        print(\"Estimator number = \" + str(estimators_num) + \"(cutoff = \" + str(cutoff) + \"): \")   \n",
        "        print(accuracy)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n6Wrw13-SXbE",
        "nbgrader": {
          "grade": false,
          "grade_id": "spec-3-2-1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### step 1.2: Try different cutoff value\n",
        "From the homework assignment, we could find when the value of cutoff is within 10 ~ 40, the accuracy would be approximately 0.79.\n",
        "\n",
        "Therefore, concerning we have taken cutoff = 1000 in homework and find the result is not satisifying, here we choose estimator = 70 and add some others possible value (within 40 ~ 1000) of cutoff. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VDAI7SAWbD1K",
        "outputId": "9b032b8e-b2e3-4aca-d0c9-6e18107555e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "\n",
        "results = evaluate_cutoffs(reviews_tfidf, reviews_dict, reviews_times, y, [40, 70, 100, 200], 70)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimator number = 70(cutoff = 40): \n",
            "0.7939\n",
            "Estimator number = 70(cutoff = 70): \n",
            "0.7955\n",
            "Estimator number = 70(cutoff = 100): \n",
            "0.793\n",
            "Estimator number = 70(cutoff = 200): \n",
            "0.78945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qBUjzTLTZom",
        "colab_type": "text"
      },
      "source": [
        "Unfortunately, there seems no improvements by changeing the cutoff values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKcCGiJrMKvQ",
        "colab_type": "text"
      },
      "source": [
        "### step 1.3: Try different value of the estimators' number\n",
        "From above results, we could find when cutoff is larger than 100, the accuracy will drop off. Also, there is no clearly increase between 40 and 70. In these result, values within 30 to 70 get relatively high results, so we still choose 40 as result and bring it into below processes. On this basis, we try to change the number of estimators and see what will happen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PDkFe3tLsY4",
        "colab_type": "code",
        "outputId": "53d31b54-991c-4752-d1d5-9e2fff1e0794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "results = evaluate_cutoffs(reviews_tfidf, reviews_dict, reviews_times, y, [40], 100)\n",
        "results = evaluate_cutoffs(reviews_tfidf, reviews_dict, reviews_times, y, [40], 200)\n",
        "results = evaluate_cutoffs(reviews_tfidf, reviews_dict, reviews_times, y, [40], 300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimator number = 100(cutoff = 40): \n",
            "0.7951\n",
            "Estimator number = 200(cutoff = 40): \n",
            "0.79845\n",
            "Estimator number = 300(cutoff = 40): \n",
            "0.79825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n01UO4AV6ZOc",
        "colab_type": "text"
      },
      "source": [
        "Considering the time consuming, we roughly try three values, 100, 500 and 1000. However, the accuracy only elevate a little. \n",
        "\n",
        "> Random forest uses bagging (picking a sample of observations rather than all of them) and random subspace method (picking a sample of features rather than all of them, in other words - attribute bagging) to grow a tree. If the number of observations is large, but the number of trees is too small, then some observations will be predicted only once or even not at all. If the number of predictors is large but the number of trees is too small, then some features can (theoretically) be missed in all subspaces used. Both cases results in the decrease of random forest predictive power. But the last is a rather extreme case, since the selection of subspace is performed at each node.\n",
        "\n",
        "In this problem, 70 has already been a reasonable value of this dataset and thus although we increase the number of estimators, the accuracy will not improve to more than 80%.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZjqSrxDYovi",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Try several frequently used models and compare their performances\n",
        "From this [paper](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf), we know, generally speaking, in a machine learning approach, it is recommended to test several models regardless of their theoretical performance, because their accuracy is dependent of the training dataset. True though, a couple of algorithms are generally preferred for text classification (SVM, Naive Bayes, multinomial regressions) for various reasons such as linear separation or curse of dimensionality (also see this [paper](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf)). Therefore, in this step, we will try several models to find if they will have better performance than the random forest model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK0gtMb797Lt",
        "colab_type": "text"
      },
      "source": [
        "### step 2.1: functions for training, predicting, evaluating model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOD24yQhATPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_classifier(clf, X_train, y_train):\n",
        "    ''' Fits a classifier to the training data. '''\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    return clf\n",
        "\n",
        "\n",
        "    \n",
        "def predict_labels(clf, X_test, y_test):\n",
        "    ''' Makes predictions using a fit classifier based on roc_auc score. '''\n",
        "\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
        "    ''' Train and predict using a classifer based on acc score. '''\n",
        "    \n",
        "    # Train the classifier\n",
        "    train_classifier(clf, X_train, y_train)\n",
        "    \n",
        "    # Print the results of prediction for both training and testing\n",
        "    print (\"ACC score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
        "    print (\"ACC score for test set: {:.4f}.\\n\".format(predict_labels(clf, X_test, y_test)))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGnJbiul9gXG",
        "colab_type": "text"
      },
      "source": [
        "### step 2.2: split the dataset into test and train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md_sj77TEUfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#split into testing and training sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XjMA8vE9rzM",
        "colab_type": "text"
      },
      "source": [
        "### step 2.3: try 3 classifiers and get their corresponding accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZO8DIXbRBz3",
        "colab_type": "code",
        "outputId": "8bb3a583-5025-4e14-9299-818673d87a6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the models using a random state were applicable.\n",
        "clf_lst = [GaussianNB(), \n",
        "            AdaBoostClassifier(), \n",
        "            LogisticRegression()\n",
        "          \n",
        "           ]\n",
        "for clf in clf_lst:\n",
        "  train_predict(clf, X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACC score for training set: 0.6421.\n",
            "ACC score for test set: 0.6423.\n",
            "\n",
            "ACC score for training set: 0.7318.\n",
            "ACC score for test set: 0.7256.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ACC score for training set: 0.7863.\n",
            "ACC score for test set: 0.7833.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaTC9Cs--LmX",
        "colab_type": "text"
      },
      "source": [
        "From the results above, we could find that these 3 models all didn't get better accuracies than the Random Forest. In next step, we will try Neural Network and see if that works well with text classification.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEnY6h-XYytJ",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0THgX0Vw_i8d",
        "colab_type": "text"
      },
      "source": [
        "We have to admit, comparing with a good old algorithms, such as Naive ones, neural networks are much more like black boxs. We are hard to explain clearly what happen inside for every steps. However, recurrent neural networks, LSTM and GRU in particular, are widely used in many natural language processing applications such as classification and language modeling. \n",
        "\n",
        "Take LSTM (Long short-term memory) as an example, it is widely used in text classifictaion, especially [emotion detection](https://www.aclweb.org/anthology/E17-2017.pdf) and so on. Therefore, in this part, we will do two things as below:\n",
        "\n",
        "\n",
        "1.   Build a very simple NN model and see if it has better performance on test data than the models before. If works, which also means\n",
        "2.   Build a LSTM (lone short-term memory) to try to hit a much higher accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btKTslTwY6TU",
        "colab_type": "text"
      },
      "source": [
        "### Step 3.1: a simple NN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jrPGiElXag2",
        "colab_type": "text"
      },
      "source": [
        "Build the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVVs40kz29X0",
        "colab_type": "code",
        "outputId": "386a87f3-251d-4a93-f9c8-a5c782f927bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "\n",
        "# Initialize keras sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Now we will create the layers for the model and add them to the sequential model\n",
        "model.add(Dense(70, activation='relu'))\n",
        "model.add(Dense(210, activation='relu'))\n",
        "\n",
        "# Add the final fully connected layer with the softmax activation function\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer= 'sgd', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv9jmmmEXeuR",
        "colab_type": "text"
      },
      "source": [
        "Fit and Check the performance of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih6_mB-QZEBz",
        "colab_type": "code",
        "outputId": "67e1b3f8-7d13-4314-c077-c7fa425be27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "y_5d = pd.get_dummies(y).as_matrix()\n",
        "\n",
        "# Fit the model to the training data and run the training for 20 epochs\n",
        "model.fit(X, y_5d, epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "100000/100000 [==============================] - 15s 148us/step - loss: 1.5407 - acc: 0.5490\n",
            "Epoch 2/100\n",
            "100000/100000 [==============================] - 10s 99us/step - loss: 1.1615 - acc: 0.7223\n",
            "Epoch 3/100\n",
            "100000/100000 [==============================] - 10s 100us/step - loss: 0.7781 - acc: 0.7592\n",
            "Epoch 4/100\n",
            "100000/100000 [==============================] - 10s 100us/step - loss: 0.6605 - acc: 0.7746\n",
            "Epoch 5/100\n",
            "100000/100000 [==============================] - 10s 100us/step - loss: 0.6287 - acc: 0.7798\n",
            "Epoch 6/100\n",
            "100000/100000 [==============================] - 10s 99us/step - loss: 0.6153 - acc: 0.7842\n",
            "Epoch 7/100\n",
            "100000/100000 [==============================] - 10s 99us/step - loss: 0.6086 - acc: 0.7859\n",
            "Epoch 8/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.6040 - acc: 0.7876\n",
            "Epoch 9/100\n",
            "100000/100000 [==============================] - 10s 98us/step - loss: 0.6004 - acc: 0.7891\n",
            "Epoch 10/100\n",
            "100000/100000 [==============================] - 10s 99us/step - loss: 0.5974 - acc: 0.7887\n",
            "Epoch 11/100\n",
            "100000/100000 [==============================] - 10s 98us/step - loss: 0.5955 - acc: 0.7893\n",
            "Epoch 12/100\n",
            "100000/100000 [==============================] - 10s 95us/step - loss: 0.5930 - acc: 0.7919\n",
            "Epoch 13/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.5912 - acc: 0.7913\n",
            "Epoch 14/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.5894 - acc: 0.7918\n",
            "Epoch 15/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5875 - acc: 0.7918\n",
            "Epoch 16/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.5866 - acc: 0.7933\n",
            "Epoch 17/100\n",
            "100000/100000 [==============================] - 10s 95us/step - loss: 0.5850 - acc: 0.7933\n",
            "Epoch 18/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5837 - acc: 0.7934\n",
            "Epoch 19/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5820 - acc: 0.7934\n",
            "Epoch 20/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.5810 - acc: 0.7942\n",
            "Epoch 21/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5796 - acc: 0.7943\n",
            "Epoch 22/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5781 - acc: 0.7965\n",
            "Epoch 23/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.5772 - acc: 0.7954\n",
            "Epoch 24/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.5755 - acc: 0.7960\n",
            "Epoch 25/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5738 - acc: 0.7968\n",
            "Epoch 26/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5728 - acc: 0.7971\n",
            "Epoch 27/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5710 - acc: 0.7971\n",
            "Epoch 28/100\n",
            "100000/100000 [==============================] - 10s 102us/step - loss: 0.5702 - acc: 0.7984\n",
            "Epoch 29/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5689 - acc: 0.7980\n",
            "Epoch 30/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.5673 - acc: 0.7995\n",
            "Epoch 31/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5661 - acc: 0.7989\n",
            "Epoch 32/100\n",
            "100000/100000 [==============================] - 10s 95us/step - loss: 0.5645 - acc: 0.7997\n",
            "Epoch 33/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5636 - acc: 0.8000\n",
            "Epoch 34/100\n",
            "100000/100000 [==============================] - 10s 95us/step - loss: 0.5623 - acc: 0.8009\n",
            "Epoch 35/100\n",
            "100000/100000 [==============================] - 10s 95us/step - loss: 0.5612 - acc: 0.8007\n",
            "Epoch 36/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.5598 - acc: 0.8005\n",
            "Epoch 37/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5591 - acc: 0.8015\n",
            "Epoch 38/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5579 - acc: 0.8023\n",
            "Epoch 39/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5567 - acc: 0.8023\n",
            "Epoch 40/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5558 - acc: 0.8021\n",
            "Epoch 41/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5543 - acc: 0.8026\n",
            "Epoch 42/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5536 - acc: 0.8041\n",
            "Epoch 43/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5520 - acc: 0.8042\n",
            "Epoch 44/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5509 - acc: 0.8038\n",
            "Epoch 45/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5499 - acc: 0.8042\n",
            "Epoch 46/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5487 - acc: 0.8040\n",
            "Epoch 47/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5483 - acc: 0.8052\n",
            "Epoch 48/100\n",
            "100000/100000 [==============================] - 10s 95us/step - loss: 0.5475 - acc: 0.8048\n",
            "Epoch 49/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5458 - acc: 0.8058\n",
            "Epoch 50/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5451 - acc: 0.8057\n",
            "Epoch 51/100\n",
            "100000/100000 [==============================] - 10s 95us/step - loss: 0.5443 - acc: 0.8060\n",
            "Epoch 52/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5433 - acc: 0.8060\n",
            "Epoch 53/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5422 - acc: 0.8070\n",
            "Epoch 54/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5413 - acc: 0.8072\n",
            "Epoch 55/100\n",
            "100000/100000 [==============================] - 9s 93us/step - loss: 0.5408 - acc: 0.8073\n",
            "Epoch 56/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5398 - acc: 0.8076\n",
            "Epoch 57/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5392 - acc: 0.8078\n",
            "Epoch 58/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5384 - acc: 0.8081\n",
            "Epoch 59/100\n",
            "100000/100000 [==============================] - 9s 93us/step - loss: 0.5373 - acc: 0.8087\n",
            "Epoch 60/100\n",
            "100000/100000 [==============================] - 10s 98us/step - loss: 0.5367 - acc: 0.8090\n",
            "Epoch 61/100\n",
            "100000/100000 [==============================] - 10s 98us/step - loss: 0.5364 - acc: 0.8088\n",
            "Epoch 62/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5350 - acc: 0.8088\n",
            "Epoch 63/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.5339 - acc: 0.8108\n",
            "Epoch 64/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5333 - acc: 0.8097\n",
            "Epoch 65/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5331 - acc: 0.8101\n",
            "Epoch 66/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5325 - acc: 0.8103\n",
            "Epoch 67/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5313 - acc: 0.8103\n",
            "Epoch 68/100\n",
            "100000/100000 [==============================] - 9s 93us/step - loss: 0.5309 - acc: 0.8113\n",
            "Epoch 69/100\n",
            "100000/100000 [==============================] - 9s 92us/step - loss: 0.5306 - acc: 0.8103\n",
            "Epoch 70/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5294 - acc: 0.8115\n",
            "Epoch 71/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5286 - acc: 0.8118\n",
            "Epoch 72/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5285 - acc: 0.8116\n",
            "Epoch 73/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5277 - acc: 0.8112\n",
            "Epoch 74/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5272 - acc: 0.8125\n",
            "Epoch 75/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5265 - acc: 0.8127\n",
            "Epoch 76/100\n",
            "100000/100000 [==============================] - 9s 93us/step - loss: 0.5260 - acc: 0.8117\n",
            "Epoch 77/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5255 - acc: 0.8124\n",
            "Epoch 78/100\n",
            "100000/100000 [==============================] - 9s 93us/step - loss: 0.5248 - acc: 0.8125\n",
            "Epoch 79/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5244 - acc: 0.8126\n",
            "Epoch 80/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5242 - acc: 0.8132\n",
            "Epoch 81/100\n",
            "100000/100000 [==============================] - 10s 95us/step - loss: 0.5235 - acc: 0.8129\n",
            "Epoch 82/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5231 - acc: 0.8131\n",
            "Epoch 83/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5226 - acc: 0.8139\n",
            "Epoch 84/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5217 - acc: 0.8147\n",
            "Epoch 85/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5218 - acc: 0.8144\n",
            "Epoch 86/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5212 - acc: 0.8136\n",
            "Epoch 87/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5209 - acc: 0.8140\n",
            "Epoch 88/100\n",
            "100000/100000 [==============================] - 10s 98us/step - loss: 0.5201 - acc: 0.8142\n",
            "Epoch 89/100\n",
            "100000/100000 [==============================] - 9s 93us/step - loss: 0.5202 - acc: 0.8143\n",
            "Epoch 90/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5194 - acc: 0.8143\n",
            "Epoch 91/100\n",
            "100000/100000 [==============================] - 10s 95us/step - loss: 0.5187 - acc: 0.8145\n",
            "Epoch 92/100\n",
            "100000/100000 [==============================] - 10s 96us/step - loss: 0.5188 - acc: 0.8144\n",
            "Epoch 93/100\n",
            "100000/100000 [==============================] - 10s 98us/step - loss: 0.5179 - acc: 0.8150\n",
            "Epoch 94/100\n",
            "100000/100000 [==============================] - 10s 98us/step - loss: 0.5178 - acc: 0.8155\n",
            "Epoch 95/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5172 - acc: 0.8155\n",
            "Epoch 96/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5173 - acc: 0.8148\n",
            "Epoch 97/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5168 - acc: 0.8146\n",
            "Epoch 98/100\n",
            "100000/100000 [==============================] - 9s 95us/step - loss: 0.5166 - acc: 0.8165\n",
            "Epoch 99/100\n",
            "100000/100000 [==============================] - 10s 97us/step - loss: 0.5165 - acc: 0.8155\n",
            "Epoch 100/100\n",
            "100000/100000 [==============================] - 9s 94us/step - loss: 0.5159 - acc: 0.8156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdb6bc592b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ4jFptKUWvn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "### step 3.2: LSTM\n",
        "\n",
        "#### step 3.2.1: import the required stuffs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl6Hh_KglTnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import nltk\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import os\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xde4w-Rexpeo",
        "colab_type": "text"
      },
      "source": [
        "Tokenization and embedding functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfmK_qRFm81i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyTokenizer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        transformed_X = []\n",
        "        for document in X:\n",
        "            tokenized_doc = []\n",
        "            for sent in nltk.sent_tokenize(document):\n",
        "                tokenized_doc += nltk.word_tokenize(sent)\n",
        "            transformed_X.append(np.array(tokenized_doc))\n",
        "        return np.array(transformed_X)\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.transform(X)\n",
        "\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(word2vec.wv.syn0[0])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = MyTokenizer().fit_transform(X)\n",
        "        \n",
        "        return np.array([\n",
        "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMyURb1_xoXw",
        "colab_type": "text"
      },
      "source": [
        "Build the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwjsw9V-APPJ",
        "colab_type": "code",
        "outputId": "a7879307-df93-48a0-c1d4-d4744021f653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "# Use the Keras tokenizer\n",
        "\n",
        "num_words = 18716 # the length of vocabulary\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(df_2w['Text'].values)\n",
        "\n",
        "# Pad the data \n",
        "X = tokenizer.texts_to_sequences(df_2w['Text'].values)\n",
        "X = pad_sequences(X, maxlen=1000)\n",
        "\n",
        "# Build out our simple LSTM\n",
        "embed_dim = 256\n",
        "lstm_out = 196\n",
        "\n",
        "# Model saving callback\n",
        "ckpt_callback = ModelCheckpoint('keras_model', \n",
        "                                 monitor='val_loss', \n",
        "                                 verbose=1, \n",
        "                                 save_best_only=True, \n",
        "                                 mode='auto')\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=18716, output_dim=256, input_length = 1000))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(256, recurrent_dropout=0.3, dropout=0.3))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(5,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 1000, 256)         4791296   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1000, 256)         0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 5,383,685\n",
            "Trainable params: 5,383,685\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFhKeZCwx8aa",
        "colab_type": "text"
      },
      "source": [
        "split the data (both x and y) into train and test and check the shape of these 4 datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH-aPj5VqP4W",
        "colab_type": "code",
        "outputId": "c8f64a92-116f-40bd-b38f-6227211b10de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "Y = pd.get_dummies(df_2w['Rate']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42, stratify=Y)\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80000, 1000) (80000, 5)\n",
            "(20000, 1000) (20000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQO09bnKrWUj",
        "colab_type": "text"
      },
      "source": [
        "fit the model and see the performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phlYp16AqnyS",
        "colab_type": "code",
        "outputId": "ad12bb7d-da0f-45ce-9bac-809a5b00570d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "batch_size = 512\n",
        "model.fit(X_train, Y_train, epochs=5, batch_size=batch_size, validation_split=0.2,callbacks=[ckpt_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 64000 samples, validate on 16000 samples\n",
            "Epoch 1/5\n",
            "64000/64000 [==============================] - 365s 6ms/step - loss: 0.7716 - acc: 0.7096 - val_loss: 0.4192 - val_acc: 0.8589\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.41923, saving model to keras_model\n",
            "Epoch 2/5\n",
            "64000/64000 [==============================] - 363s 6ms/step - loss: 0.3405 - acc: 0.8866 - val_loss: 0.3994 - val_acc: 0.8628\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.41923 to 0.39943, saving model to keras_model\n",
            "Epoch 3/5\n",
            "64000/64000 [==============================] - 365s 6ms/step - loss: 0.2729 - acc: 0.9088 - val_loss: 0.4151 - val_acc: 0.8622\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.39943\n",
            "Epoch 4/5\n",
            "64000/64000 [==============================] - 365s 6ms/step - loss: 0.2283 - acc: 0.9222 - val_loss: 0.4321 - val_acc: 0.8596\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.39943\n",
            "Epoch 5/5\n",
            "64000/64000 [==============================] - 366s 6ms/step - loss: 0.2018 - acc: 0.9303 - val_loss: 0.4682 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.39943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd5fe45e160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3WNH8NuT57U",
        "colab_type": "text"
      },
      "source": [
        "Load the model for tracking and training future"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CieCIXnk8Yub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model('keras_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxF7KyGZT4mK",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-9DyDvHK29a",
        "colab_type": "code",
        "outputId": "d1d79404-0dd9-4371-e7f1-0e50180faaac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000/20000 [==============================] - 454s 23ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38772718875408174, 0.8687]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXc5nn7PO2Et",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Influence of reviews' time \n",
        "In this step, we will try to find if there exist any connection betweeen time and rate of reviews. Maybe customer will tend to give lower rate in different time Take myself as example, I will tend to give bad feedback when I am upset and I usually feel upset before finals :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qb7f2RwRtiU",
        "colab_type": "text"
      },
      "source": [
        "#### 4.1 Prepare the data for visualization\n",
        "\n",
        "In this part, we will do following steps:\n",
        "1. First, convert the raw data to timestrap\n",
        "2. Change the X, y into dataframe form\n",
        "3. Get two new columns, one is year for date and the other is month of date, and this will help us to easily figure out the distribution of the counts of rank by year and by month\n",
        "4. sort the dataframe so that we could see the start year of reviews and the end year of reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yzcn3otaT6B",
        "colab_type": "code",
        "outputId": "bace14d0-d10f-4497-a874-8d8605a43089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# convert the raw data to timestrap\n",
        "reviews_date = convert_times(reviews_times)\n",
        "\n",
        "# change the X, y into dataframe form#\n",
        "import pandas as pd\n",
        "d = {\"Rate\": y, \"Date\" : reviews_date}\n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "# add two new columns(year, month)\n",
        "year_list = [date.year for date in reviews_date]\n",
        "month_list = [date.month for date in reviews_date]\n",
        "df.insert(2, \"Year\", year_list)\n",
        "df.insert(3, \"Month\", month_list)\n",
        "\n",
        "# show\n",
        "display(df)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rate</th>\n",
              "      <th>Date</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2012-05-06</td>\n",
              "      <td>2012</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2014-04-07</td>\n",
              "      <td>2014</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2012-07-17</td>\n",
              "      <td>2012</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2014-07-14</td>\n",
              "      <td>2014</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2013-05-16</td>\n",
              "      <td>2013</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>5</td>\n",
              "      <td>2013-04-04</td>\n",
              "      <td>2013</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>5</td>\n",
              "      <td>2010-09-21</td>\n",
              "      <td>2010</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>5</td>\n",
              "      <td>2013-05-12</td>\n",
              "      <td>2013</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>5</td>\n",
              "      <td>2013-03-05</td>\n",
              "      <td>2013</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>5</td>\n",
              "      <td>2012-07-27</td>\n",
              "      <td>2012</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Rate       Date  Year  Month\n",
              "0         1 2012-05-06  2012      5\n",
              "1         1 2014-04-07  2014      4\n",
              "2         1 2012-07-17  2012      7\n",
              "3         1 2014-07-14  2014      7\n",
              "4         1 2013-05-16  2013      5\n",
              "...     ...        ...   ...    ...\n",
              "99995     5 2013-04-04  2013      4\n",
              "99996     5 2010-09-21  2010      9\n",
              "99997     5 2013-05-12  2013      5\n",
              "99998     5 2013-03-05  2013      3\n",
              "99999     5 2012-07-27  2012      7\n",
              "\n",
              "[100000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cBYO0hDFXSm",
        "colab_type": "text"
      },
      "source": [
        "By sorting the data by column \"Date\", we could find that the reviews was started from 2001 and end in 2014"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZNXeP9wfO7O",
        "colab_type": "code",
        "outputId": "7411dab9-e746-48b4-eede-55b9af42936b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# sort the date \n",
        "df_sorted = df.sort_values(by='Date')\n",
        "display(df_sorted)\n",
        "# count the number of 2001 to check the sorted result\n",
        "sum_2001 = df.loc[df.Year == 2001, \"Year\"].count()\n",
        "print(sum_2001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rate</th>\n",
              "      <th>Date</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34557</th>\n",
              "      <td>2</td>\n",
              "      <td>2001-07-17</td>\n",
              "      <td>2001</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32949</th>\n",
              "      <td>2</td>\n",
              "      <td>2001-07-25</td>\n",
              "      <td>2001</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26512</th>\n",
              "      <td>2</td>\n",
              "      <td>2001-08-24</td>\n",
              "      <td>2001</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97988</th>\n",
              "      <td>5</td>\n",
              "      <td>2002-01-01</td>\n",
              "      <td>2002</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5061</th>\n",
              "      <td>1</td>\n",
              "      <td>2002-01-05</td>\n",
              "      <td>2002</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73698</th>\n",
              "      <td>4</td>\n",
              "      <td>2014-07-22</td>\n",
              "      <td>2014</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80178</th>\n",
              "      <td>5</td>\n",
              "      <td>2014-07-22</td>\n",
              "      <td>2014</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95901</th>\n",
              "      <td>5</td>\n",
              "      <td>2014-07-22</td>\n",
              "      <td>2014</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89116</th>\n",
              "      <td>5</td>\n",
              "      <td>2014-07-23</td>\n",
              "      <td>2014</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69294</th>\n",
              "      <td>4</td>\n",
              "      <td>2014-07-23</td>\n",
              "      <td>2014</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Rate       Date  Year  Month\n",
              "34557     2 2001-07-17  2001      7\n",
              "32949     2 2001-07-25  2001      7\n",
              "26512     2 2001-08-24  2001      8\n",
              "97988     5 2002-01-01  2002      1\n",
              "5061      1 2002-01-05  2002      1\n",
              "...     ...        ...   ...    ...\n",
              "73698     4 2014-07-22  2014      7\n",
              "80178     5 2014-07-22  2014      7\n",
              "95901     5 2014-07-22  2014      7\n",
              "89116     5 2014-07-23  2014      7\n",
              "69294     4 2014-07-23  2014      7\n",
              "\n",
              "[100000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQXO0SFWCWpH",
        "colab_type": "text"
      },
      "source": [
        "###4.2 Visualization of the couts of reviews by month and year\n",
        "\n",
        "In this part, we did follwwing steps:\n",
        "1. Plot out the bar graph by year(x-axis) and the count of rates in corresponding year (y-axis)\n",
        "2. Plot out the bar graph by month(x-axis) and the count of rates in corresponding year (y-axis)\n",
        "3. In the beginning, we have uniform rating labels, 20000 for each rate(1, 2, 3, 4, 5). Therefore, we would like to see if there is some hints among years. For example, maybe people in the old days will be stricter with products and would give lower rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5P3QdTZMFy7",
        "colab_type": "text"
      },
      "source": [
        "#### 4.2.1 Plot\n",
        "\n",
        "1. Counts of reviews' number by year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_tKFYMVvXPC",
        "colab_type": "code",
        "outputId": "46af76eb-3f81-4da0-e11e-e0ced33bbde4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "groups_year = df.groupby(['Year']).size()\n",
        "groups_year.plot.bar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd5fde836a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVgMJElYDCRJWAwkScD/B41tHnJHb9j6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqznl3UzNCga",
        "colab_type": "text"
      },
      "source": [
        "2. Counts of reviews' numbers by with month"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Praba5Tj2uc",
        "colab_type": "code",
        "outputId": "57ac3847-361c-4e16-8b85-a8f99552c09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "groups_year = df.groupby(['Month']).size()\n",
        "groups_year.plot.bar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd5fd5cea58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEJCAYAAAB2T0usAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCcNAkoRhIEkC/he+33GHp6CkFAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4Svm2C2NHUq",
        "colab_type": "text"
      },
      "source": [
        "3. Check the distribution for different ratings after 2010"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZE4NKkNkZBp",
        "colab_type": "code",
        "outputId": "5484f079-397a-455e-bf83-8e902f24b884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "groups_rate_early = df[df[\"Year\"]> 2010].groupby(['Rate']).size()\n",
        "groups_rate_early.plot.bar()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd5fd5ce1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEDCAYAAADX1GjKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7reo4olStTZ",
        "colab_type": "text"
      },
      "source": [
        "4. check the distribution for different ratings befor 2010"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMp4OpiOlThU",
        "colab_type": "code",
        "outputId": "d1112cbf-35ad-41c3-958a-853f305011c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "groups_rate_late = df[df[\"Year\"]<= 2010].groupby(['Rate']).size()\n",
        "groups_rate_late.plot.bar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd5fd4cd5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEGCAYAAACJnEVTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7R5PhMCSsEx",
        "colab_type": "text"
      },
      "source": [
        "5. check the average ratings by year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPBLjNtjDcDK",
        "colab_type": "code",
        "outputId": "1e62a03f-a24a-470d-d58c-a02d57723771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "df_year_averaged = df.groupby([\"Year\"]).mean()\n",
        "df_year_averaged[\"Rate\"].plot.bar()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd5fd521ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHwT+AankBPRcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX4N6STdSq1q",
        "colab_type": "text"
      },
      "source": [
        "6. check the average ratings by month"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM3BnlOXSCpz",
        "colab_type": "code",
        "outputId": "62ce8137-f032-4e0a-f543-3f568c5ea659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "df_year_averaged = df.groupby([\"Month\"]).mean()\n",
        "df_year_averaged[\"Rate\"].plot.bar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd5fd3d1208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEJCAYAAACE39xMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQa6JDXi/wBTNqbepLK1XgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuOzL3Pt95Xf",
        "colab_type": "text"
      },
      "source": [
        "From the plots above, we could find that the influence from different years might be larger than by different months. The reason is the total number of different ratings seems similar if we analysize it by month. Therefore, in the next part, we will try to add the year column to the previous dataset (the one with 1000 dimensions after PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_93IPvNGSxP5",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 Add the year column into the training dataset\n",
        "\n",
        "From the analyse above, we could find that there does have influence from date, especially from year. Therefore, in the part, we will add the year information to the x dataset. To achieve this goal, we need to do:\n",
        "\n",
        "\n",
        "1.   standarize the year column\n",
        "2.   add the year column to the tfidf data\n",
        "3.   retrain the random forest model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1fgWF3q7QXh",
        "colab_type": "text"
      },
      "source": [
        "#### 4.3.1: standarize the year column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzLSu54xem3F",
        "colab_type": "code",
        "outputId": "45d7f39c-a34c-4fc8-fc63-2f24f7e0c52d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "df[[\"Year\"]] = preprocessing.StandardScaler().fit_transform(df[[\"Year\"]])\n",
        "x_year = df[\"Year\"].to_numpy()\n",
        "print(X.shape)\n",
        "print(x_year.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 1000)\n",
            "(100000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAB9hU597Xn6",
        "colab_type": "text"
      },
      "source": [
        "#### 4.3.2: add the year column to the TF-IDF data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rY1X9WG7K7d",
        "colab_type": "code",
        "outputId": "31ec3076-f323-4bc7-92b1-54ef21bb8d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "X_final = np.vstack((X.T, x_year.T)).T\n",
        "print(X_final.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 1001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbcyZF2L7i4g",
        "colab_type": "text"
      },
      "source": [
        "#### 4.3.3: use the 1001 dimensions new data to retrain the best LSTMs model\n",
        "\n",
        "Since last time we use 2 epochs to get a pretty good model and the training after that did not improve any more, also, considering the time consuming, we would only train 3 epochs just for comparing the influence. Hope a good result :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XGffg_Q7Lt5",
        "colab_type": "code",
        "outputId": "55f9618a-993e-4b2b-d4d2-262785cb8b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "Y = pd.get_dummies(df_2w['Rate']).values\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_final, Y, test_size = 0.2, random_state = 42, stratify=Y)\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80000, 1001) (80000, 5)\n",
            "(20000, 1001) (20000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3i2W5s3mCdt",
        "colab_type": "code",
        "outputId": "02f663ad-2fc8-43b3-cad7-2c0cc9dff4a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "# Use the Keras tokenizer\n",
        "\n",
        "num_words = 18716 # the length of vocabulary\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(df_2w['Text'].values)\n",
        "\n",
        "# Pad the data \n",
        "X = tokenizer.texts_to_sequences(df_2w['Text'].values)\n",
        "X = pad_sequences(X, maxlen=1000)\n",
        "\n",
        "# Build out our simple LSTM\n",
        "embed_dim = 256\n",
        "lstm_out = 196\n",
        "\n",
        "# Model saving callback\n",
        "ckpt_callback = ModelCheckpoint('keras_model', \n",
        "                                 monitor='val_loss', \n",
        "                                 verbose=1, \n",
        "                                 save_best_only=True, \n",
        "                                 mode='auto')\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(input_dim=18716, output_dim=256, input_length = 1001))\n",
        "model2.add(Dropout(0.3))\n",
        "model2.add(LSTM(256, recurrent_dropout=0.3, dropout=0.3))\n",
        "model2.add(Dense(256,activation='relu'))\n",
        "model2.add(Dropout(0.1))\n",
        "model2.add(Dense(5,activation='softmax'))\n",
        "model2.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "print(model2.summary())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 1001, 256)         4791296   \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 1001, 256)         0         \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 5,383,685\n",
            "Trainable params: 5,383,685\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI8XVVZ3mkkL",
        "colab_type": "code",
        "outputId": "18f57558-8c8d-4664-aa72-1e2a23735b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "batch_size = 512\n",
        "model2.fit(X_train, Y_train, epochs=3, batch_size=batch_size, validation_split=0.2,callbacks=[ckpt_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 64000 samples, validate on 16000 samples\n",
            "Epoch 1/3\n",
            "64000/64000 [==============================] - 381s 6ms/step - loss: 1.6065 - acc: 0.2123 - val_loss: 1.5978 - val_acc: 0.2204\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.59781, saving model to keras_model\n",
            "Epoch 2/3\n",
            "64000/64000 [==============================] - 378s 6ms/step - loss: 1.5972 - acc: 0.2299 - val_loss: 1.5956 - val_acc: 0.2368\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.59781 to 1.59563, saving model to keras_model\n",
            "Epoch 3/3\n",
            "64000/64000 [==============================] - 384s 6ms/step - loss: 1.5963 - acc: 0.2333 - val_loss: 1.5951 - val_acc: 0.2368\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.59563 to 1.59507, saving model to keras_model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd5fc73c2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKBq1D3Cnehi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = load_model('keras_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjHr0t4vnf7y",
        "colab_type": "code",
        "outputId": "c592b41a-3861-4faf-bb7b-d111613b41ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "model2.evaluate(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000/20000 [==============================] - 462s 23ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5934274448394776, 0.2377]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kmrs3HR7MaN",
        "colab_type": "text"
      },
      "source": [
        "Unfortunately, the result did not improve the model and this seems the year was not improtant as we thought. Another reason for this might be the distribution is imbalanced. For example, people bought more things recent years. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTO4gyKjwsmn",
        "colab_type": "text"
      },
      "source": [
        "##Summary:\n",
        "\n",
        "The aim of this project was to do some quick exploration of the dataset and apply some common ML techniques to the classification task and see which one might have the best accuracy in test dataset.\n",
        "\n",
        "A big part of the problem is to teach an ML model how to \"read\" reviews literature and classify the given reviews' content and reviews' time into 1 out of 5 rating classes.\n",
        "\n",
        "After preparing necessary methods and data in section 0. In the first part of this project, we inherit the Random Forest model from the hw4 and try to improving it by doing more hyperparameter tunning. However, this idea didn't work well and improve the accuracy.\n",
        "\n",
        "Thus, the second part of this notebook focused on applying common techniques in ML totrain and predict reviews' eatings and evaluate their effectiveness by evalauting their accuracy. However, all the results did not show much improvement.\n",
        "\n",
        "\n",
        "> The classifiers used, from least effective to most effective, were: Logistic Regression > AdaBoost Classifier > Gaussian Naive Bayes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Because the above approaches did not take into account the seuqences in reviews' text, incluing the random forest, their accuracies seem not so good.\n",
        "\n",
        "In the third part of this notebook, we tried two neural networks. The first one is a simple NN and after 100 epochs, we found the accuracy did improvement (up to more than 81.5%) and which was a good promising direction. Then we built a more complex LSTM model, which is frequently used in text classification and got pretty high accuracy of *more than 86.87%.*\n",
        "\n",
        "In the last part of the notebook, after comparing the possible influence from year of date and month of date, I added the \"Year\" feature next to the previous 1000 features( the one we got after cutoff as 1000). I tried it on our best model LSTM, and found that the results did not improve and dropped. \n",
        "\n"
      ]
    }
  ]
}